diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 04de8d366..fa1330d27 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -93,6 +93,21 @@ class Worker(WorkerBase):
         else:
             self.profiler = None
 
+        # [GVM] Stream for execution, to be compatible with XSched
+        self.exec_stream = torch.cuda.Stream()
+
+    @staticmethod
+    def with_exec_stream(method):
+        # [GVM] for xsched to work
+        from functools import wraps
+
+        @wraps(method)
+        def wrapper(self, *args, **kwargs):
+            with torch.cuda.stream(self.exec_stream):
+                return method(self, *args, **kwargs)
+
+        return wrapper
+
     def sleep(self, level: int = 1) -> None:
         from vllm.device_allocator.cumem import CuMemAllocator
 
@@ -349,6 +364,7 @@ class Worker(WorkerBase):
         return self.model_runner.get_supported_tasks()
 
     @torch.inference_mode()
+    @with_exec_stream  # [GVM] for xsched to work
     def execute_model(
         self,
         scheduler_output: "SchedulerOutput",
